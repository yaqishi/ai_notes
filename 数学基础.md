# 线性代数

## 标量、向量、矩阵、张量

定义：

- 标量（scalar）：一个单独的数
- 向量（vector）：一列数
- 矩阵（matrix）：二维数组
- 张量（tensor）：超过二维的数组

操作：

- 转置（transpose）：$(A^T)_{i,j=A_{j,i}}$
- 广播（broadcasting）：$C_{i,j}=A_{i,j}+b_j$
- 矩阵乘法（matrix multiply)：$C_{i,j}=\sum_kA_{i, k}B_{k, j}$
- 哈达玛积（hadamard product）：即逐元素积，$C=A\odot B$
- 点积（dot product）：长度相同的向量，$<x, y>=x^Ty$

==Einsum==：爱因斯坦求和约定

基本概念：

- 自由索引（Free Indices）：箭头右边的索引
- 求和索引（Summation Indices）：只出现在箭头左边的索引

基本规则：

- 箭头左边，不同输入之间重复出现的索引：沿该维度做乘法操作
- 只出现在箭头左边的索引：计算结果在这个维度上求和
- 箭头右边的索引顺序可以任意，代表计算完成在相应维度的转置

特殊规则：

- 可以不写包括箭头在内的右边部分，输出张量的维度会根据默认规则推导
- 表达式支持省略号，表示不关心的索引。如最后两维的转置：$\dots ij\rightarrow \dots ji$

实例：

- 提取主对角线元素：$ii\rightarrow i$
- 矩阵转置：$ij\rightarrow ji$
- 求和：$ij\rightarrow$
- 列求和：$ij\rightarrow j$
- 行求和：$ij\rightarrow i$
- 矩阵-向量相乘：$ik,k\rightarrow i$
- 矩阵-矩阵相乘：$ik,kj\rightarrow ij$
- 向量点积：$i,i\rightarrow$
- 矩阵点积：$ij,ij\rightarrow$
- 哈达玛积：$ij,ij\rightarrow ij$
- 外积：$i,j\rightarrow ij$
- batch矩阵相乘：$ijk,ikl\rightarrow ikl$
- 张量缩约：$pqrs,tuqvr\rightarrow pstuv$

## 范数

范数（norm）：衡量向量大小。$L^p$范数定义为：
$$
\Vert x\Vert_p=(\sum_i|x_i|^p)^{\frac{1}{p}}
$$

- $f(x)=0\rightarrow x=0$
- $f(x+y)\le f(x)+f(y)$
- $\forall\alpha\in\mathbb R, f(\alpha x)=|\alpha|f(x)$

常用$L^p$范数：

- $p=2$时，$L^2$范数称欧几里得范数（Euclidean norm）衡量从原点出发到向量$x$确定的点的欧几里得距离。十分常用，但原点附近增长缓慢。
- $p=1$时，$L^1$范数$\Vert x\Vert_1=\sum_i|x_i|$。当零和非零元素之间的差异非常重要时，会使用$L^1$范数
- $p=\infty$时，$L^\infty$范数$\Vert x\Vert_\infty=\max_i|x_i|$称最大范数，表示向量中具有最大幅值的元素的绝对值。
- Forbenius范数：衡量矩阵的大小。$\Vert A\Vert_F=\sqrt{\sum_{i, j}A_{i, j}^2}$
- 两向量的点积可以用范数表示：$x^Ty=\Vert x\Vert_2\Vert y\Vert_2\cos\theta$

## 特殊矩阵和向量

- 对角矩阵（diagonal matrix）$D$：$\forall i\not=j, D_{i,j}=0$
- 单位矩阵（identity matrix）$I_n\in\mathbb R^{n\times n}$：主对角线1，其余位置0
- 对称（symmetric）：$A=A^T$
- 单位向量（unit vector）：具有单位范数的向量$\Vert x\Vert_2=1$
- 正交（orthogonal）：$x^Ty=0$。如果两个向量都有非零范数，那么其夹角为90°。$\mathbb R^n$中至多有$n$个非零范数单位向量相互正交，称标准正交（orthonormal）。
- 逆矩阵（matrix inversion）：$A^{-1}A=AA^{-1}=I_n$
- 正交矩阵（orthogonal matrix）：$A^TA=AA^T=I$，即$A^{-1}=A^T$

## 矩阵分解

### 特征分解

方阵$A$的特征向量（eigenvector）指与A相乘后相当于对该向量进行缩放的非零向量$v$：
$$
Av=\lambda v
$$
标量$\lambda$称为这个特征向量对应的特征值（eigenvalue）。

- 如果$v$是$A$的特征向量，$v$缩放后的任意向量$sv(s\in\mathbb R, s\not=0)$也是$A$的特征向量，且与$v$有相同的特征值。所以通常只考虑单位特征向量
- 假设$A$有$n$个线性无关的特征向量$\{v^{(1)}, \dots, v^{(n)}\}$，对应特征值$\{\lambda_1, \dots, \lambda_n\}$，则将特征向量拼接成一个矩阵，使得每一列是一个特征向量$\mathbf V=[v^{(1)}, \dots, v^{(n)}]$，将特征值连接成一个向量$\mathbf\lambda=[\lambda_1, \dots, \lambda_n]^T$，$A$的特征分解可以记作：

$$
A=Vdiag(\mathbf\lambda)V^{-1}
$$

如果方阵$A$的元素都为实数，且矩阵的转置等于本身，即$a_{ij}=a_{ji}$，称$A$为实对称矩阵。每个实对称矩阵都可以分解成实特征向量和实特征值：
$$
A=Q\Lambda Q^T
$$
$Q$是$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵，特征值$\Lambda_{ii}$对应的特征向量是$Q$的第$i$列，记作$Q_{:,i}$

- 所有特征值都是正数的矩阵称为正定（positive definite）：$x^TAx=0\Rightarrow x=0$
- 所有特征值都是非负数的矩阵称为半正定（positive semidefinite）：$\forall x, x^TAx\ge0$
- 所有特征值都是负数的矩阵称为负定（negative definite）
- 所有特征值都是非正数的矩阵称为半负定（negative semidefinite）

### 奇异值分解

奇异值分解（singular value decomposition, SVD）：将矩阵分解为奇异向量（singular vector）和奇异值（singular value）。

假设$A$是一个$m\times n$的矩阵，可以分解为三个矩阵的乘积：
$$
A=UDV^T
$$

- $U^{m\times m}$的列向量为左奇异向量（left singular vector），是$AA^T$的特征向量
- $V^{n\times n}$的列向量为右奇异向量（right singular vector），是$A^TA$的特征向量
- $D^{m\times n}$是对角矩阵，对角线上的元素称为矩阵$A$的奇异值。$A$的非零奇异值是$A^TA$特征值的平方根，同时也是$AA^T$特征值的平方根。

### Moore-Penrose伪逆

非方阵没有逆矩阵，在$Ax=y$中，希望通过$A$的左逆$B$解线性方程，即$x=By$

矩阵$A$的伪逆定义为：
$$
A^+=\lim_{\alpha\rightarrow0}(A^TA+\alpha I)^{-1}A^T
$$
计算公式为：
$$
A^+=VD^+U^T
$$
其中$U,D,V$是矩阵$A$奇异值分解得到的矩阵，$D$的伪逆$D^+$是非零元素取倒数再转置得到的。

- $A$的列数多于行数时，用伪逆求解线性方程是一种解法。$x=A^+y$是方程所有可行解中欧几里得范数$\Vert x\Vert_2$最小的一个
- $A$的行数多于列数时，可能没有解。通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离$\Vert Ax-y\Vert_2$最小

## 迹运算

迹运算：返回矩阵对角元素的和。
$$
\mathrm{Tr}(A)=\sum_iA_{i,i}
$$
性质和作用：

- 在转置运算下不变：$\mathrm{Tr}(A)=\mathrm{Tr}(A^T)$
- 多个矩阵相乘的迹，在矩阵乘积依然定义良好的情况下，将最后一个矩阵挪到最前面后，迹不变：$\mathrm{Tr}(ABC)=\mathrm{Tr}(CAB)=\mathrm{Tr}(BCA)$
- 即使循环后矩阵乘积的形状变了，迹的结果仍然不变：$\mathrm{Tr}(AB)=\mathrm{Tr}(BA)$，尽管$AB\in\mathbb R^{m\times m}$且$BA\in\mathbb R^{n\times n}$
- 求Frobenius范数：$\Vert A\Vert_F=\sqrt{\mathrm{Tr}(AA^T)}$

## 行列式

记作$\mathrm{det}(A)$，将一个方阵$A$映射到实数。
$$
\mathrm{det}(A)=\Pi\lambda_i
$$
行列式的绝对值衡量矩阵参与乘法后空间扩大或缩小了多少。如果行列式为0，空间至少沿着某一维完全收缩了。如果行列式是1，则这个转换保持空间体积不变。

# 概率与信息论

